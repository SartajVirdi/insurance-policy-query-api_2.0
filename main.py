# -*- coding: utf-8 -*-
"""V2_Bajaj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iWd9sHaFTqMc2ivEqNu-FCOufDQLeC_8
"""

from flask import Flask, request, jsonify
from huggingface_hub import login
from transformers import BitsAndBytesConfig
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    SimpleDirectoryReader,
    PromptTemplate
)
import torch
import os

app = Flask(__name__)

# ✅ Hugging Face login via env variable
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
if HUGGINGFACE_TOKEN:
    login(token=HUGGINGFACE_TOKEN)

# ✅ Load policy documents from policies/ folder
documents = SimpleDirectoryReader("policies").load_data()

# ✅ Prompts
system_prompt = """
You are a decision-support assistant. Answer user questions clearly and concisely based on insurance policies, contracts, and email instructions.
Always cite specific clauses from the documents to support your answer.
Respond conversationally and directly, like:
"Yes, knee surgery is covered under the policy (Clause 3.2)."
or
"No, the claim is not valid because pre-existing conditions are excluded (Clause 5.1)."
"""

query_wrapper_prompt = PromptTemplate(
    "<|USER|>Question: {query_str}\n\nRespond in JSON with a decision, justification, and clause reference.<|ASSISTANT|>"
)

# ✅ Falcon-7B-Instruct Setup
quant_config = BitsAndBytesConfig(load_in_8bit=True)
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=512,
    generate_kwargs={
        "temperature": 0.3,
        "do_sample": True,
        "repetition_penalty": 1.1
    },
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="tiiuae/falcon-7b-instruct",
    model_name="tiiuae/falcon-7b-instruct",
    device_map="auto",
    model_kwargs={
        "torch_dtype": torch.float16,
        "quantization_config": quant_config
    }
)

# ✅ Embeddings
embed_model = HuggingFaceEmbedding(model_name="intfloat/e5-large-v2")

# ✅ Apply Settings
Settings.llm = llm
Settings.embed_model = embed_model

# ✅ Build index & engine
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# ✅ Webhook route for querying
@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.get_json()
    if not data or "query" not in data:
        return jsonify({"error": "Missing 'query' field"}), 400

    try:
        response = query_engine.query(data["query"])
        return jsonify({"response": str(response)})
    except Exception as e:
        return jsonify({"error": f"Query failed: {e}"}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)